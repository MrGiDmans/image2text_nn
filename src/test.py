from __future__ import annotations

import argparse

import torch
from PIL import Image
from torchvision import transforms

from models import CaptioningModel, DecoderRNN, EncoderCNN
from utils.vocabulary import Vocabulary


def load_model(
    checkpoint_path: str,
    vocab_path: str,
    device: torch.device,
) -> tuple[CaptioningModel, Vocabulary]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Å–ª–æ–≤–∞—Ä—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞."""
    checkpoint = torch.load(checkpoint_path, map_location=device)
    vocab = Vocabulary.load(vocab_path)

    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏
    encoder = EncoderCNN(
        encoded_image_size=14,
        embed_dim=512,
        fine_tune=False,
    )
    decoder = DecoderRNN(
        vocab_size=len(vocab),
        embed_size=512,
        decoder_dim=512,
        encoder_dim=512,
        dropout=0.5,
    )
    model = CaptioningModel(encoder, decoder, vocab, device=device)
    model.load_state_dict(checkpoint["model_state"])
    model.eval()
    return model, vocab


def preprocess_image(image_path: str, image_size: int = 224) -> torch.Tensor:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏."""
    transform = transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(image_size),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
            ),
        ]
    )

    image = Image.open(image_path).convert("RGB")
    image_tensor = transform(image).unsqueeze(0)  # [1, 3, H, W]
    return image_tensor


def generate_caption(
    model: CaptioningModel,
    image_path: str,
    device: torch.device,
    max_len: int = 20,
    beam_size: int = 3,
    mode: str = "greedy",
) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–¥–ø–∏—Å—å –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    image_tensor = preprocess_image(image_path).to(device)

    with torch.no_grad():
        results = model.generate(
            image_tensor, max_len=max_len, beam_size=beam_size, mode=mode
        )

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ (—É–±–∏—Ä–∞–µ–º BOS/EOS)
    seq_idx, seq_words, score = results[0]
    words = [w for w in seq_words if w not in ["<bos>", "<eos>", "<pad>", "<unk>"]]
    caption = " ".join(words)
    return caption


def main():
    parser = argparse.ArgumentParser(description="–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ–¥–ø–∏—Å–µ–π")
    parser.add_argument(
        "--checkpoint",
        type=str,
        default="./checkpoints/best_checkpoint.pth.tar",
        help="–ü—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É –º–æ–¥–µ–ª–∏",
    )
    parser.add_argument(
        "--vocab",
        type=str,
        default="./vocab.pkl",
        help="–ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é",
    )
    parser.add_argument(
        "--image",
        type=str,
        required=True,
        help="–ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
    )
    parser.add_argument(
        "--max_len",
        type=int,
        default=20,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º–æ–π –ø–æ–¥–ø–∏—Å–∏",
    )
    parser.add_argument(
        "--beam_size",
        type=int,
        default=3,
        help="–†–∞–∑–º–µ—Ä beam –¥–ª—è beam search",
    )
    parser.add_argument(
        "--mode",
        type=str,
        default="greedy",
        choices=["greedy", "beam"],
        help="–†–µ–∂–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: greedy –∏–ª–∏ beam",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞",
    )

    args = parser.parse_args()

    device = torch.device(args.device)
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {args.checkpoint}...")
    model, vocab = load_model(args.checkpoint, args.vocab, device)
    print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(vocab)}")

    print(f"\n–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥–ø–∏—Å–∏ –¥–ª—è {args.image}...")
    caption = generate_caption(
        model,
        args.image,
        device,
        max_len=args.max_len,
        beam_size=args.beam_size,
        mode=args.mode,
    )

    print(f"\nüì∑ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {args.image}")
    print(f"üìù –ü–æ–¥–ø–∏—Å—å: {caption}")
    print(f"üîß –†–µ–∂–∏–º: {args.mode}")


if __name__ == "__main__":
    main()

